{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834d24eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'Qwen2.5-3B-Instruct'...\n",
      "Filtering content: 100% (2/2)\n",
      "Filtering content: 100% (2/2), 5.74 GiB | 17.35 MiB/s\n",
      "Filtering content: 100% (2/2), 5.74 GiB | 7.80 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "# !git clone https://huggingface.co/Qwen/Qwen2.5-3B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cd2aced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed！Save as converted_train.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def convert_json_file(input_file, output_file):\n",
    "    try:\n",
    "        with open(input_file, \"r\", encoding=\"utf-8\") as infile, open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "            for line in infile:\n",
    "                try:\n",
    "                    item = json.loads(line.strip())\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Skip invalid line: {line.strip()}\")\n",
    "                    continue\n",
    "\n",
    "                # 提取原始数据\n",
    "                question = item.get(\"question\", \"\")\n",
    "                exp = item.get(\"exp\", \"\")\n",
    "                cop = item.get(\"cop\", 0)\n",
    "                opa = item.get(\"opa\", \"\")\n",
    "                opb = item.get(\"opb\", \"\")\n",
    "                opc = item.get(\"opc\", \"\")\n",
    "                opd = item.get(\"opd\", \"\")\n",
    "                subject_name = item.get(\"subject_name\", \"\")\n",
    "                topic_name = item.get(\"topic_name\", \"\")\n",
    "                choice_type = item.get(\"choice_type\", \"\")\n",
    "\n",
    "                # 确定正确答案\n",
    "                correct_option_letter = \"\"\n",
    "                correct_option = \"\"\n",
    "                if cop == 1:\n",
    "                    correct_option_letter = \"A.\"\n",
    "                    correct_option = opa\n",
    "                elif cop == 2:\n",
    "                    correct_option_letter = \"B.\"\n",
    "                    correct_option = opb\n",
    "                elif cop == 3:\n",
    "                    correct_option_letter = \"C.\"\n",
    "                    correct_option = opc\n",
    "                elif cop == 4:\n",
    "                    correct_option_letter = \"D.\"\n",
    "                    correct_option = opd\n",
    "\n",
    "                # 如果没有解释，则用正确答案填充\n",
    "                if not exp:\n",
    "                    exp = correct_option\n",
    "\n",
    "                # 更新解释字段\n",
    "                updated_exp = f\"The correct answer is: {correct_option_letter} {correct_option}. {exp}\"\n",
    "\n",
    "                # 构造新的字典\n",
    "                converted_item = {\n",
    "                    \"instruction\": f\"Answer the following {choice_type}-choice questions which From the subject of {subject_name}, questions about the {topic_name}, provide the correct answers and explanations\",\n",
    "                    \"input\": f\"{question}\\nA. {opa}\\nB. {opb}\\nC. {opc}\\nD. {opd}\",\n",
    "                    \"output\": updated_exp\n",
    "                }\n",
    "\n",
    "                # 将转换后的数据逐行写入输出文件\n",
    "                json.dump(converted_item, outfile, ensure_ascii=False)\n",
    "                outfile.write('\\n')\n",
    "\n",
    "        print(f\"Completed！Save as {output_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_path = \"train.json\"\n",
    "    output_path = \"converted_train.jsonl\"\n",
    "    convert_json_file(input_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c3b855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install swanlab\n",
    "import swanlab\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from modelscope import snapshot_download, AutoTokenizer\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    TrainerCallback,\n",
    ")\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ---------------------------- 初始化 SwanLab ----------------------------\n",
    "swanlab.login(api_key=\"YH3XHJl5mRRIwco61Cp96\", save=True)\n",
    "\n",
    "run = swanlab.init(\n",
    "    project=\"Qwen2.5-LoRA-Law\",\n",
    "    experiment_name=\"3b-2\",\n",
    "    config={\n",
    "        # 实验配置\n",
    "        \"model\": \"https://modelscope.cn/models/Qwen/Qwen2.5-3B-Instruct\",\n",
    "        \"dataset\": \"https://drive.google.com/uc?export=download&id=15VkJdq5eyWIkfb_aoD3oS8i4tScbHYky\",\n",
    "        \"github\": \"https://github.com/PeaceChoy/Medical-Questions-Answering-via-Fine-tuned-Qwen2.5-3B-Mode\",\n",
    "        \"system_prompt\": \"You are a medical expert. Please provide professional answers based on the users' questions.\",\n",
    "        # LoRA 配置\n",
    "        \"lora_rank\": 8,\n",
    "        \"lora_alpha\": 32,\n",
    "        \"lora_dropout\": 0.1,\n",
    "        # 训练参数\n",
    "        \"per_device_train_batch_size\": 4,\n",
    "        \"per_device_eval_batch_size\": 4,\n",
    "        \"gradient_accumulation_steps\": 4,\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"num_train_epochs\": 2,\n",
    "        \"evaluation_strategy\": \"steps\",\n",
    "        \"eval_steps\": 60,\n",
    "    },\n",
    ")\n",
    "\n",
    "# ---------------------------- 数据处理函数 ----------------------------\n",
    "def process_func(example):\n",
    "    MAX_LENGTH = 384\n",
    "    instruction = tokenizer(\n",
    "        f\"<|im_start|>system\\n{example['instruction']}<|im_end|>\\n<|im_start|>user\\n{example['input']}<|im_end|>\\n<|im_start|>assistant\\n\",\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "    response = tokenizer(f\"{example['output']}\", add_special_tokens=False)\n",
    "    \n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]\n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "\n",
    "    if len(input_ids) > MAX_LENGTH:\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "        \n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "\n",
    "# ---------------------------- 预测函数 ----------------------------\n",
    "def predict(messages, model, tokenizer):\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=512)\n",
    "    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]\n",
    "    \n",
    "    return tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "# ---------------------------- 验证损失计算函数 ----------------------------\n",
    "def compute_validation_loss(model, val_dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            total_loss += loss.item() * len(batch[\"input_ids\"])\n",
    "            total_samples += len(batch[\"input_ids\"])\n",
    "            \n",
    "    avg_loss = total_loss / total_samples\n",
    "    model.train()\n",
    "    return avg_loss\n",
    "\n",
    "# ---------------------------- 自定义回调 ----------------------------\n",
    "class PredictionCallback(TrainerCallback):\n",
    "    def __init__(self, test_df, model, tokenizer):\n",
    "        self.test_df = test_df\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        print(\"\\n训练开始，初始预测：\")\n",
    "        test_text_list = []\n",
    "        for index, row in self.test_df[:3].iterrows():\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": row[\"instruction\"]},\n",
    "                {\"role\": \"user\", \"content\": row[\"input\"]},\n",
    "            ]\n",
    "            response = predict(messages, self.model, self.tokenizer)\n",
    "            result_text = f\"【Q】{row['input']}\\n【LLM】{response}\\n\"\n",
    "            test_text_list.append(swanlab.Text(result_text, caption=response))\n",
    "            print(result_text)\n",
    "        swanlab.log({\"Prediction\": test_text_list}, step=0)\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        print(f\"\\nEpoch {int(state.epoch)} 结束，开始预测：\")\n",
    "        test_text_list = []\n",
    "        for index, row in self.test_df.iterrows():\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": row[\"instruction\"]},\n",
    "                {\"role\": \"user\", \"content\": row[\"input\"]},\n",
    "            ]\n",
    "            response = predict(messages, self.model, self.tokenizer)\n",
    "            result_text = f\"【Q】{row['input']}\\n【LLM】{response}\\n【GT】{row['output']}\"\n",
    "            test_text_list.append(swanlab.Text(result_text, caption=response))\n",
    "            if index == 0: print(result_text)  # 只打印第一条\n",
    "        swanlab.log({\"Prediction\": test_text_list}, step=int(state.epoch))\n",
    "\n",
    "class ValidationCallback(TrainerCallback):\n",
    "    def __init__(self, val_dataloader, model, eval_steps=60):\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.model = model\n",
    "        self.eval_steps = eval_steps\n",
    "        \n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % self.eval_steps == 0:\n",
    "            val_loss = compute_validation_loss(self.model, self.val_dataloader, self.model.device)\n",
    "            print(f\"\\nStep {state.global_step}: Validation Loss = {val_loss:.4f}\")\n",
    "            swanlab.log({\"val_loss\": val_loss}, step=state.global_step)\n",
    "\n",
    "class SwanLabCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None:\n",
    "            swanlab.log({\n",
    "                \"loss\": logs.get(\"loss\", None),\n",
    "                \"learning_rate\": logs.get(\"learning_rate\", None),\n",
    "                \"grad_norm\": logs.get(\"grad_norm\", None)\n",
    "            }, step=state.global_step)\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 加载模型\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"./Qwen2.5-3B-Instruct/\", use_fast=False, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"./Qwen2.5-3B-Instruct/\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "    model.enable_input_require_grads()\n",
    "\n",
    "    # 准备数据集\n",
    "    all_data = pd.read_json(\"./converted_train.jsonl\", lines=True)\n",
    "    train_df = all_data[:5000]\n",
    "    test_df = all_data[5001:5006]\n",
    "    val_df = all_data[5006:5021]  # 取15条作为验证集\n",
    "    \n",
    "    train_dataset = Dataset.from_pandas(train_df).map(process_func, remove_columns=train_df.columns.tolist())\n",
    "    val_dataset = Dataset.from_pandas(val_df).map(process_func, remove_columns=val_df.columns.tolist())\n",
    "    \n",
    "    # 创建验证数据加载器\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True)\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=4,\n",
    "        collate_fn=data_collator\n",
    "    )\n",
    "\n",
    "    # LoRA 配置\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        inference_mode=False,\n",
    "    )\n",
    "    peft_model = get_peft_model(model, peft_config)\n",
    "\n",
    "    # 训练参数\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./new_output/Qwen2.5-3b\",\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        logging_steps=10,\n",
    "        num_train_epochs=2,\n",
    "        save_steps=100,\n",
    "        learning_rate=1e-4,\n",
    "        gradient_checkpointing=True,\n",
    "        report_to=\"none\",  # 禁用默认的日志记录\n",
    "        logging_dir=\"./logs\",  # 添加日志目录\n",
    "        lr_scheduler_type=\"cosine\",  # 使用余弦退火调度器\n",
    "        warmup_ratio=0.1,  # 预热期占总步数的比例，这里设为10%\n",
    "    )\n",
    "\n",
    "    # 初始化训练器\n",
    "    trainer = Trainer(\n",
    "        model=peft_model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        data_collator=data_collator,\n",
    "        callbacks=[\n",
    "            PredictionCallback(test_df, peft_model, tokenizer),\n",
    "            SwanLabCallback(),\n",
    "            ValidationCallback(val_dataloader, peft_model, eval_steps=60),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # 开始训练\n",
    "    trainer.train()\n",
    "    swanlab.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
